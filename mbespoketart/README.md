# Models

Pretraining BERT with masked LM task and Switchboard and EdAcc dataset

After One Epoch:
- train loss: 3.6070